{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "data_df = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 및 테스트 데이터 세트 결합\n",
    "train_test_data = [train, test]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Title\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name값에서 성별 정보 추출\n",
    "# 정규표현식으로 [문자]. 으로 끝나는 문자열 추출\n",
    "for dataset in train_test_data:\n",
    "    dataset['Title'] = dataset['Name'].str.extract('([A-za-z]+)\\.', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_mapping = {\n",
    "      'Mr' : 0\n",
    "    , 'Miss' : 1\n",
    "    , 'Mrs' : 2\n",
    "    , 'Master' : 3\n",
    "    , 'Dr' : 4\n",
    "    , 'Rev' : 4\n",
    "    , 'Mlle' : 4\n",
    "    , 'Major' : 4\n",
    "    , 'Col' : 4\n",
    "    , 'Countess' : 4\n",
    "    , 'Capt' : 4\n",
    "    , 'Ms' : 4\n",
    "    , 'Sir' : 4\n",
    "    , 'Lady' : 4\n",
    "    , 'Mme' : 4\n",
    "    , 'Don' : 4\n",
    "    , 'Jonkheer' : 4\n",
    "}\n",
    "train['Title'] = train['Title'].map(title_mapping)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_mapping = {\n",
    "      'Mr' : 0 \n",
    "    , 'Miss' : 1\n",
    "    , 'Mrs' : 2\n",
    "    , 'Master' : 3 \n",
    "    , 'Ms' : 4\n",
    "    , 'Col' : 4\n",
    "    , 'Rev' : 4\n",
    "    , 'Dr' : 4\n",
    "    , 'Dona' : 4\n",
    "}\n",
    "test['Title']= test['Title'].map(title_mapping)\n",
    "test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 성별\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성별에 숫자 매핑\n",
    "sex_mapping = {'male': 0, 'female':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문으로 매핑결과 데이터에 적용\n",
    "for dataset in train_test_data:\n",
    "    dataset['Sex'] = dataset['Sex'].map(sex_mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### groupsize\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticket in train['Ticket'].unique():\n",
    "    train.loc[train['Ticket']==ticket,'group_size'] = len(train[train['Ticket']==ticket])\n",
    "\n",
    "for ticket in test['Ticket'].unique():\n",
    "    test.loc[test['Ticket']==ticket,'group_size'] = len(test[test['Ticket']==ticket])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Fare'] = train['Fare'] / train['group_size']\n",
    "test['Fare'] = test['Fare'] / test['group_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['group_size'].value_counts(), test['group_size'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data = [train,test]\n",
    "\n",
    "for dataset in train_test_data:\n",
    "    dataset.loc[dataset['group_size'] == 1, 'group_size'] = 0\n",
    "    dataset.loc[dataset['group_size'] == 2, 'group_size'] = 0.4\n",
    "    dataset.loc[(dataset['group_size'] == 3) | (dataset['group_size'] == 4), 'group_size'] = 0.8\n",
    "    dataset.loc[dataset['group_size'] > 4, 'group_size'] = 1.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## age\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['Age'].isnull(), 'Null_Age'] = 0\n",
    "test.loc[test['Age'].isnull(), 'Null_Age'] = 0\n",
    "\n",
    "train.loc[train['Age'].notnull(), 'Null_Age'] = 1\n",
    "test.loc[test['Age'].notnull(), 'Null_Age'] = 1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data set\n",
    "train['Age'].fillna(train.groupby('Title')['Age'].transform('mean'), inplace=True)\n",
    "\n",
    "# test data set\n",
    "test['Age'].fillna(test.groupby('Title')['Age'].transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in train_test_data:\n",
    "    dataset.loc[dataset['Age'] <= 17, 'Age'] =0\n",
    "    dataset.loc[(dataset['Age'] >17) & (dataset['Age'] <= 24), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] >24) & (dataset['Age'] <= 34), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] >34) & (dataset['Age'] <= 44), 'Age'] = 3\n",
    "    dataset.loc[(dataset['Age'] >44) & (dataset['Age'] <= 60), 'Age'] = 4\n",
    "    dataset.loc[dataset['Age'] >60, 'Age'] = 5\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나이대별 생사여부 확인\n",
    "survived = train[train['Survived']==1]['Age'].value_counts()\n",
    "dead = train[train['Survived']==0]['Age'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Embarked\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 좌석별 승선 항구 확인하기\n",
    "Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts()\n",
    "Pclass2 = train[train['Pclass']==2]['Embarked'].value_counts()\n",
    "Pclass3 = train[train['Pclass']==3]['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame으로 만들어 인덱스 주기 \n",
    "df = pd.DataFrame([Pclass1, Pclass2, Pclass3])\n",
    "df.index = ['1st class', '2nd class', '3rd class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in train_test_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝 Classifier를 위해 텍스트 숫자 변경(매핑)\n",
    "embarked_mapping = {'S':0, 'C':1, 'Q':2}\n",
    "\n",
    "# map 함수 사용해서 처리\n",
    "for dataset in train_test_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Fare\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 탑승권 가격이 결측값일 경우, 좌석 등급별 중간값으로 대치\n",
    "# train data set\n",
    "train[\"Fare\"].fillna(train.groupby('Pclass')['Fare'].transform('median'), inplace=True)\n",
    "\n",
    "# test data set\n",
    "test[\"Fare\"].fillna(test.groupby('Pclass')['Fare'].transform('median'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data = [train,test]\n",
    "for dataset in train_test_data:\n",
    "    dataset.loc[dataset['Fare'] <= 7,'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7) & (dataset['Fare'] <= 8.8), 'Fare'] = 0.4\n",
    "    dataset.loc[(dataset['Fare'] > 8.8) & (dataset['Fare'] <= 17), 'Fare'] = 0.8\n",
    "    dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 1.2\n",
    "    dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 1.6\n",
    "    dataset.loc[dataset['Fare'] > 100,'Fare'] = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Cabin\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['Cabin'].isnull(), 'Null_Cabin'] = 0\n",
    "test.loc[test['Cabin'].isnull(), 'Null_Cabin'] = 0\n",
    "\n",
    "train.loc[train['Cabin'].notnull(), 'Null_Cabin'] = 1\n",
    "test.loc[test['Cabin'].notnull(), 'Null_Cabin'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문을 통해 객실번호의 알파벳과 숫자 분리 후, 알파벳만 뽑아오기\n",
    "for dataset in train_test_data:\n",
    "    dataset['Cabin'] = dataset['Cabin'].str[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스별로 객실 종류 count\n",
    "Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts()\n",
    "Pclass2 = train[train['Pclass']==2]['Cabin'].value_counts()\n",
    "Pclass3 = train[train['Pclass']==3]['Cabin'].value_counts()\n",
    "\n",
    "df = pd.DataFrame([Pclass1, Pclass2, Pclass3])\n",
    "df.index = ['1st class', '2nd class', '3rd class']\n",
    "\n",
    "df.plot(kind='bar', stacked=True, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier를 위해 매핑\n",
    "# feature scaling : raw data 전처리하는 과정 (feature들의 크기, 범위 정규화)/ 소수점 사용\n",
    "# 숫자의 범위가 비슷하지 않으면 먼 거리에 있는 데이터를 조금 더 중요하게 생각할 수 있음 주의\n",
    "\n",
    "cabin_mapping = {'A':0, 'B':0.4, 'C':0.8, 'D':1.2, 'E':1.6, 'F':2, 'G':2.4, 'T': 2.8}\n",
    "\n",
    "for dataset in train_test_data:\n",
    "    dataset['Cabin'] = dataset['Cabin'].map(cabin_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cabin의 missing field는 1등급 2등급 3등급 클래스와 밀접한 관계\n",
    "# # fillna\n",
    "train['Cabin'].fillna(\n",
    "        train.groupby('Pclass')['Cabin'].transform('median')\n",
    "    ,   inplace=True\n",
    ")\n",
    "test['Cabin'].fillna(\n",
    "        test.groupby('Pclass')['Cabin'].transform('median')\n",
    "    ,   inplace=True\n",
    ")\n",
    "train.isnull().sum(), test.isnull().sum()\n",
    "# train.tail(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Family Size\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혼자타면 SibSp, Parch 모두 0으로 표시되므로 +1 해주기\n",
    "train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n",
    "test['FamilySize'] = test['SibSp'] + test['Parch'] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Family_Survival\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Lastname'] = data_df['Name'].apply(lambda x: str.split(x, \",\")[0])\n",
    "\n",
    "default_survival_value = 0.5\n",
    "data_df['Family_Survival'] = default_survival_value\n",
    "\n",
    "for group, group_df in data_df[['Survived','Name', 'Lastname', 'Fare', 'Ticket', 'PassengerId',\n",
    "                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Lastname', 'Fare']):\n",
    "    if (len(group_df) != 1):\n",
    "        for ind, row in group_df.iterrows():\n",
    "            smax = group_df.drop(ind)['Survived'].max()\n",
    "            smin = group_df.drop(ind)['Survived'].min()\n",
    "            passID = row['PassengerId']\n",
    "            if (smax == 1.0):\n",
    "                data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 1\n",
    "            elif (smin==0.0):\n",
    "                data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 0\n",
    "\n",
    "    data_df.loc[data_df['Family_Survival']!=0.5].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, group_df in data_df.groupby('Ticket'):\n",
    "    if (len(group_df) != 1):\n",
    "        for ind, row in group_df.iterrows():\n",
    "            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n",
    "                smax = group_df.drop(ind)['Survived'].max()\n",
    "                smin = group_df.drop(ind)['Survived'].min()\n",
    "                passID = row['PassengerId']\n",
    "                if (smax == 1.0):\n",
    "                    data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 1\n",
    "                elif (smin==0.0):\n",
    "                    data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 0\n",
    "                        \n",
    "train['Family_Survival'] = data_df['Family_Survival'][:891]\n",
    "test['Family_Survival'] = data_df['Family_Survival'][891:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_mapping = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}\n",
    "train['FamilySize'] = train['FamilySize'].map(family_mapping)\n",
    "test['FamilySize'] = test['FamilySize'].map(family_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 컬럼 정리\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요한 데이터 삭제 : drop\n",
    "# Ticket, SibSp, Parch, PassengerId 정보 제거\n",
    "\n",
    "features_drop = ['Ticket', 'SibSp', 'Parch', 'Name']\n",
    "\n",
    "train = train.drop(features_drop, axis=1)\n",
    "test = test.drop(features_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 정규화\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Pclass', 'Sex', 'Age', 'Fare', 'Cabin',\n",
    "       'Embarked', 'Title', 'Null_Age', 'group_size', 'Null_Cabin',\n",
    "       'FamilySize','Family_Survival']\n",
    "# 객체 생성\n",
    "scaler = MinMaxScaler()\n",
    "# 데이터 셑 변환, fit(), transform()\n",
    "scaler.fit(train[columns])\n",
    "scaled = scaler.transform(train[columns])\n",
    "\n",
    "#transforma()시 스케일 변환된 데이터 세트가 ndarray로 반환돼 이를 DataFrame으로 변환\n",
    "df_train = pd.DataFrame(data=scaled, columns=columns)\n",
    "\n",
    "print('최솟값')             # 0에 가까워짐\n",
    "print(df_train.min())\n",
    "print('\\n최댓값')\n",
    "print(df_train.max()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Pclass', 'Sex', 'Age', 'Fare', 'Cabin',\n",
    "       'Embarked', 'Title', 'Null_Age', 'group_size', 'Null_Cabin',\n",
    "       'FamilySize','Family_Survival']\n",
    "# 객체 생성\n",
    "scaler = MinMaxScaler()\n",
    "# 데이터 셑 변환, fit(), transform()\n",
    "scaler.fit(test[columns])\n",
    "scaled = scaler.transform(test[columns])\n",
    "\n",
    "#transforma()시 스케일 변환된 데이터 세트가 ndarray로 반환돼 이를 DataFrame으로 변환\n",
    "df_test = pd.DataFrame(data=scaled, columns=columns)\n",
    "\n",
    "print('최솟값')             # 0에 가까워짐\n",
    "print(df_test.min())\n",
    "print('\\n최댓값')\n",
    "print(df_test.max()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:,columns] = df_train\n",
    "test.loc[:,columns] = df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### train survived 분리\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.drop('Survived',axis=1)\n",
    "target = train['Survived']\n",
    "\n",
    "train_data.shape, target.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### modeling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import xgboost as xgb \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "clf = KNeighborsClassifier(n_neighbors=13) # Knn Model object\n",
    "scoring = 'accuracy' #평가지표 : 정확도\n",
    "score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=-1, scoring=scoring)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "clf = DecisionTreeClassifier() # Knn Model object\n",
    "scoring = 'accuracy' #평가지표 : 정확도\n",
    "score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=-1, scoring=scoring)\n",
    "print(score.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### random_forest fitting 코드\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listr = []\n",
    "# for r in range(2,300,1):\n",
    "#     clf = RandomForestClassifier(\n",
    "#                               max_depth=4\n",
    "#                               ,n_estimators=69\n",
    "#                              ,min_samples_leaf=32\n",
    "#                              , random_state=993\n",
    "#                              ,n_jobs=-1\n",
    "#                                  )\n",
    "#     clf.fit(train_data, target) # 학습\n",
    "#     pred = clf.predict(test) # 테스트 데이터로 예측값 추출    \n",
    "#     submission1 = pd.read_csv('../답/submission (1).csv')\n",
    "#     del submission1['PassengerId']\n",
    "#     from sklearn.metrics import accuracy_score\n",
    "#     accuracy = accuracy_score(pred, submission1)\n",
    "#     print(r,accuracy)\n",
    "#     listr.append(accuracy)\n",
    "# print(listr.index(max(listr)),max(listr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 하이퍼파라미터찾기 베이지안\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission1 = pd.read_csv('../답/submission (1).csv')\n",
    "# del submission1['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hyperopt import hp , STATUS_OK\n",
    "\n",
    "# rf_search_space = {'max_depth': hp.quniform('max_depth', 1,30,1), \n",
    "#                     'random_state': hp.quniform('random_state', 1,1000,1),\n",
    "#                     'n_estimators': hp.quniform('n_estimators', 1,1000,1),\n",
    "#                     'min_samples_leaf': hp.quniform('min_samples_leaf', 1,30,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective_func(search_space):\n",
    "#     rf_clf = RandomForestClassifier(n_estimators=int(search_space['n_estimators'])\n",
    "#                             , max_depth=int(search_space['max_depth'])\n",
    "#                             ,random_state=int(search_space['random_state'])\n",
    "#                             ,min_samples_leaf=int(search_space['min_samples_leaf'])\n",
    "#                            )\n",
    "#     rf_clf.fit(train_data , target)\n",
    "#     pred = rf_clf.predict(test)\n",
    "#     submission1 = pd.read_csv('../답/submission (1).csv')\n",
    "#     del submission1['PassengerId']\n",
    "#     from sklearn.metrics import accuracy_score\n",
    "#     accuracy = accuracy_score(pred, submission1)\n",
    "#     return {'loss' : -accuracy, 'status' : STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hyperopt import fmin, tpe, Trials\n",
    "\n",
    "# trials = Trials()\n",
    "\n",
    "# # fmin()함수를 호출. max_evals지정된 횟수만큼 반복 후 목적함수의 최소값을 가지는 최적 입력값 추출.\n",
    "# best = fmin(fn=objective_func,\n",
    "#             space=rf_search_space,\n",
    "#             algo=tpe.suggest,\n",
    "#             max_evals=3000, # 최대 반복 횟수를 지정합니다.\n",
    "#             trials=trials)\n",
    "#             # rstate=np.random.default_rng(seed=))\n",
    "# print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### randomForest\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(\n",
    "                               max_depth=4\n",
    "                              ,n_estimators=69\n",
    "                             ,min_samples_leaf=32\n",
    "                             , random_state=993\n",
    "                                 )\n",
    "clf.fit(train_data, target) # 학습\n",
    "pred = clf.predict(test) # 테스트 데이터로 예측값 추출"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 컬럼 관련도\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance 추출\n",
    "\n",
    "print(\"Feature importances:\\n{0}\".format(np.round(clf.feature_importances_,3)))\n",
    "\n",
    "# feature 별 importance 매핑\n",
    "\n",
    "for name, value in zip(train_data.columns\n",
    "                       ,clf.feature_importances_):\n",
    "    print('{0} : {1:.3f}'.format(name, value))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### random_forest 파일저장\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.DataFrame(\n",
    "#     {\n",
    "#         \"PassengerId\":test[\"PassengerId\"], # 앞에서 PassendgerId 삭제했으므로 다시 불러 옴\n",
    "#         \"Survived\": pred\n",
    "#     }\n",
    "# )\n",
    "# submission.to_csv('../result/titanic83.7.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 정확도 평가 캐글에 올리지 않아도 됨\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1 = pd.read_csv('../답/submission (1).csv')\n",
    "del submission1['PassengerId']\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(pred, submission1)\n",
    "accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Xgboost\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost  = XGBClassifier()\n",
    "# xgboost.fit(train_data, target)\n",
    "# scoring = 'accuracy'\n",
    "# score = cross_val_score(xgboost, train_data, target, cv=k_fold, n_jobs=-1, scoring=scoring)\n",
    "# Y_pred = xgboost.predict(test)\n",
    "# print(score.mean())\n",
    "\n",
    "# submission = pd.DataFrame({\n",
    "#         \"PassengerId\": test[\"PassengerId\"],\n",
    "#         \"Survived\": Y_pred\n",
    "#     })\n",
    "# submission.to_csv('../result/titanic-3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost  = XGBClassifier()\n",
    "# xgboost.fit(train_data, target)\n",
    "# Y_pred = xgboost.predict(test)\n",
    "# submission1 = pd.read_csv('./답/submission (1).csv')\n",
    "# del submission1['PassengerId']\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# accuracy = accuracy_score(Y_pred, submission1)\n",
    "# print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
